{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the relationship between the customers' rating and returning, consider the curve below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9888d8c87d1f42fb8d90283f07004331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverinfo': 'none',\n",
       "              'marker': {'color': [blue, blue, blue, blue, b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from content import *\n",
    "\n",
    "fig_binom, _ = binom_likelihood(X1, y1, slope=3, intercept=0)\n",
    "fig_binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve is shaped by a so-called \"sigmoid\" function. We won't talk about the details about the sigmoid function but essentially a logistic regression is created by passing a straight line into a sigmoid function. So the coefficients of a straight line, the intercept and the slope, are still relevant even you don't see a line in appearance.\n",
    "\n",
    "For your information:\n",
    "- The intercept translates the curve.\n",
    "- The slope decide how steep the curve raises.\n",
    "\n",
    "Keep in mind that we talk about the likelihood function because we would like to evaluate how well a model predict. Consider another model  with `slope=20`. Which model do you think performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de0d283bcfe43b6828213d86760bc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverinfo': 'none',\n",
       "              'marker': {'color': [blue, blue, blue, blue, b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_binom, _ = binom_likelihood(X1, y1, slope=20, intercept=0)\n",
    "fig_binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A model that agrees more with the facts is better!**\n",
    "\n",
    "That being said, a better model should:\n",
    "\n",
    "**predict high probability that positive observations are positive, and high probability that negative observations are negative.**\n",
    "\n",
    "But this is easy to decide only for a single point. There is often a trade-off when it comes to the whole dataset. If we look at around `x=0.5`. If you predict high probability you harm the negative ones nearby and vice versa. We need a good quantitative way to strike a balance.\n",
    "\n",
    "In our example, the extent a model agrees with the facts is its prediction of **probability that everyone does what he/she does.** We use the notation \n",
    "\n",
    "$$P_x = P(Y=1 | X=x)$$\n",
    "\n",
    ". Therefore:\n",
    "\n",
    "So the probablity that we see the whole dataset is:\n",
    "$$\\color{red}{\\prod_{\\text{customers returned}} P_{\\text{customers returned}}} \\times \\color{blue}{\\prod_{\\text{customers left}} (P_{\\text{customers left}})} = \\color{red}{\\prod_{y=1} P_x} \\times \\color{blue}{\\prod_{y=0} (1-P_x)}$$\n",
    "\n",
    "So here we see that to compute **likelihood**, we need a **model** and a **dataset**. The likelihood is really how likely a model predict a dataset occurs.\n",
    "\n",
    "**Remark**:\n",
    "\n",
    "With or without a dataset, the model actually gave us a distribution attached to any x. This way, we can take a model for a collection of parameterized distributions. \n",
    "\n",
    "$$P(Y=1 | X=x) = \\frac{\\exp(\\beta_0 + \\beta_1 x)}{ 1 + \\exp(\\beta_0 + \\beta_1 x)}$$\n",
    "\n",
    "Once we are provided a dataset, the model can decide how likely we see each point, and the product tell us how likely we see the whole dataset. This can be applied to any binary classification model, even if you have a very sophisticated deep learning model. A different model may predict probabilities with a different formula so the right hand side of the equation may be different. But the formula of the likelihood function, which depends on the probabilties a model predicts regardless how they were computed, remained unchanged. \n",
    "\n",
    "\n",
    "The so-called **maximal likelihood estimate** is to choose the model that predicts the highest likelihood, compared with all the other possible pairs of slope and intercept, the dataset occurs. How does maximization work is a different story.\n",
    "\n",
    "The idea can be generalized to a linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
