{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Function for Linear Regression\n",
    "\n",
    "We would like to extend the idea of likelihood function to linear regression. Consider the following scatter plot and our favorite regression line obtained with the ordinary least estimate (OLS) that minimises the residual sum of square. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e8159a609b42a595a541cd0eec9b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverinfo': 'none',\n",
       "              'marker': {'color': [rgba(.0,.0,.6,0.5), rgba(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from content import *\n",
    "\n",
    "gaussian_likelihood(2, 1, X2, y2, 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss another possible approach with likelihood function.\n",
    "\n",
    "\n",
    "Let's consider a bad model below - why is it bad?\n",
    "\n",
    "Of course, we know that the residuals are too big and it seems quite easy to improve. But that's our old good OLS approach.\n",
    "\n",
    "Recall the assumptions of lineaer regression. If we would like to, just like what we did for logistic regression, attach a distribution to every point of your regression line. What kind of distributions would you use? And how would you attach them?\n",
    "\n",
    "That will then give us a collection of parametrised distribution. What does that tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cc36a469b0470fa3f6785187bbb185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverinfo': 'none',\n",
       "              'marker': {'color': [rgba(.0,.0,.6,0.5), rgba(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " gaussian_likelihood(1, 1, X2, y2, 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model below looks better, but is it better everywhere?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4a05e186ed4060abb7eabb3719b70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverinfo': 'none',\n",
       "              'marker': {'color': [rgba(.0,.0,.6,0.5), rgba(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaussian_likelihood(2, 1, X2, y2, 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, a different model might improve the prediction at a certain point but harm others. We need to find an overall best performing model.\n",
    "\n",
    "Each model predicts how likely we see a point, which is:\n",
    "\n",
    "$$ p(Y=y| X=x)$$\n",
    "\n",
    "The number above represents: for $X=x$, how likely is $Y=y$. The product of all these numbers of probability density is the likelihood that a model predicts that the dataset occur. The full formula is:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^n p(Y=y_i| X=x_i) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\times \\exp( - (\\frac{y_i - \\hat{y_i}}{2 \\sigma})^2 )\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\hat{y_i} = \\beta_0 + \\beta_1 x_i $$\n",
    "\n",
    "The coefficients that maximized this likelihood is the estimators we select. For linear regression, it is well-known that the maximal likelihood estimators coincide the OLS.\n",
    "\n",
    "**Side Remark**:\n",
    "\n",
    "Notice that the Gaussian distribution gives us probability density, instead of probability. We won't discuss the details about probability and probability density, we just list a few facts that you may encounter:\n",
    "\n",
    "- Probability density is not probability. It is not ranged from 0 to 1.\n",
    "- Probability density still quantify likelohood -- higher probability density means more likely to happen.\n",
    "- The area beneath the probability density curve is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
