<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Evaluating Classification Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Carlos Afonso – Data Science Instructor  NYC Data Science Academy" />
    <meta name="date" content="2021-09-01" />
    <script src="presentation_files/header-attrs-2.8/header-attrs.js"></script>
    <link href="presentation_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="presentation_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="presentation_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Evaluating Classification Models
### Carlos Afonso – Data Science Instructor<br><br>NYC Data Science Academy
### <br>September 1, 2021

---


# Outline, Objectives, Prerequisities

.pull-left[

### Outline / Objectives

Introduction: Machine Learning, Classification

(Binary) Classification

- 4 Outcome Types &amp; 2 Error Types
- Confusion Matrix (*Activity, Code*)
- Evaluation Metrics
  - Confusion Matrix (*Activity, Code*)
      - Accuracy, Precision, Recall, Specificity
  - ROC curve &amp; ROC AUC
  - Precision-Recall Curve

Summary &amp; Extra Notes

- Imbalanced Classification
- Multiclass Classification

]

--

.pull-right[

### Prerequisities

**Required:** Pen (or pencil) and paper.

**Good to have (*but not required*):**

- Some knowledge of Machine Learning and Classification.

- Understand R and/or Python.

]

---

# Introduction

.pull-left[

.center[

*Machine Learning vs Traditional Programming*

&lt;img src="figures/Traditional-Programming-vs-Machine-Learning.png" width="65%" height="65%"&gt;

]

**Machine Learning (ML):** learning from data

- *Unsupervised ML:* learning from *unlabeled* data
- **Supervised ML:** learning from **labeled** data
  - *Regression:* predict a *numeric* variable
  - **Classification:** predict a **categorical** variable

&lt;p style="font-size:70%; font-style: italic;"&gt;Image credits: Pedro Domingos (left), Edureka (right)&lt;/p&gt;

]

--

.pull-right[

.center[

*Machine Learning Process*

&lt;img src="figures/Machine-Learning-Process-Edureka.png" width="80%" height="80%"&gt;

]

**Model Evaluation:** How to evaluate a Classification model?
- By comparing **predicted** vs **actual** outputs

**Classification:**
- **Binary** or *Multiple* classes
- **Balanced** or *Imbalanced* classes

]

---

# 4 Outcome Types &amp; 2 Error Types

| `\(y_{actual}\)` | `\(y_{predicted}\)` | *Outcome Type* | *Meaning* | *Example* |
| :----------: | :-------------: | -------------- | --------- | --------- |
| 1 | 1 | `\(\color{green}{\text{True Positive}}\)`   | &lt;span style="font-size:80%"&gt;Model **correctly** predicts the **positive** (1) class.&lt;/span&gt; | &lt;span style="font-size:80%"&gt;Model predicts **cancer (1)** *and* it's actually **cancer (1)**.&lt;/span&gt;         |
| 0 | 0 | `\(\color{blue}{\text{True Negative}}\)`    | &lt;span style="font-size:80%"&gt;Model **correctly** predicts the **negative** (0) class.&lt;/span&gt; | &lt;span style="font-size:80%"&gt;Model predicts **not cancer (0)** *and* it's actually **not cancer (0)**.&lt;/span&gt; |
| 0 | 1 | `\(\color{orange}{\text{False Positive}}\)` | &lt;span style="font-size:80%"&gt;Model **wrongly** predicts the **positive** (1) class.&lt;/span&gt;   | &lt;span style="font-size:80%"&gt;Model predicts **cancer (1)** *but* it's actually **not cancer (0)**.&lt;/span&gt;     |
| 1 | 0 | `\(\color{red}{\text{False Negative}}\)`    | &lt;span style="font-size:80%"&gt;Model **wrongly** predicts the **negative** (0) class.&lt;/span&gt;   | &lt;span style="font-size:80%"&gt;Model predicts **not cancer (0)** *but* it's actually **cancer (1)**.&lt;/span&gt;     |

.pull-left[

**Actual / Predicted:**

* `\(y_{actual}\)`: the actual/observed/real class.
* `\(y_{predicted}\)`: the class predicted by the model.

***Outcome Type:***

* **True / False:** model prediction is *correct (True)* or *wrong (False)*, as compared to the actual class.
* **Positive / Negative:** the two classes in binary classification: *Positive (1)* or *Negative (0)* class.

]

.pull-right[

**The 2 Error Types**

* *Type I error:* `\(\color{orange}{\text{False Positive}}\)`
* *Type II error:* `\(\color{red}{\text{False Negative}}\)`

***What's the worst error type?***

* It depends on the nature of the problem at hand.
* In the cancer example a `\(\color{red}{\text{False Negative}}\)` is potentially worse than a `\(\color{orange}{\text{False Positive}}\)`.

]

---

# Confusion Matrix

.left-40[

.center[**Model Results on Test Dataset (S = 10)**]
  
| `\(y_{actual}\)` | `\(y_{predicted}\)` | *Outcome Type*                               |
| ------------ | --------------- | -------------------------------------------- |
|            1 |               1 | `\(\color{green}{\text{True Positive (TP)}}\)`   |
|            0 |               0 | `\(\color{blue}{\text{True Negative (TN)}}\)`    |
|            0 |               1 | `\(\color{orange}{\text{False Positive (FP)}}\)` |
|            1 |               0 | `\(\color{red}{\text{False Negative (FN)}}\)`    |
|            0 |               0 | `\(\color{blue}{\text{TN}}\)`   |
|            1 |               1 | `\(\color{green}{\text{TP}}\)`  |
|            0 |               0 | `\(\color{blue}{\text{TN}}\)`   |
|            0 |               1 | `\(\color{orange}{\text{FP}}\)` |
|            0 |               0 | `\(\color{blue}{\text{TN}}\)`   |
|            1 |               1 | `\(\color{green}{\text{TP}}\)`  |

]

.right-60[

.center[**Confusion Matrix**]

| *Counts*                                  | Predicted Positive &lt;br&gt; `\(y_{predicted} = 1\)`  | Predicted Negative &lt;br&gt; `\(y_{predicted} = 0\)` |
| :---------------------------------------: | :------------------------------------------: | :-----------------------------------------: |
| **Actual Positive** &lt;br&gt; `\(y_{actual} = 1\)` | `\(\color{green}{\text{#TP = 3}}\)`  | `\(\color{red}{\text{#FN = 1}}\)`   |
| **Actual Negative** &lt;br&gt; `\(y_{actual} = 0\)` | `\(\color{orange}{\text{#FP = 2}}\)` | `\(\color{blue}{\text{#TN = 4}}\)`   |

- Shows counts of correct and wrong results by outcome type
- Fundamental tool to evaluate classification model quality
- Most classification metrics are derived from Confusion Matrix
- Can be computed for the training and testing datasets
  - Train/Test results can be compared to check for overfitting
  - Ultimately, the test dataset results are the decisive ones
- *Reversed* version: **Actual** in columns and **Predicted** in rows
- *Ratios* version: Counts / Total (e.g., `\(\color{green}{\text{#TP/S = 3/10 = 0.3}}\)`)

]

---

# Confusion Matrix - Activity (5 min)

.left-40[

.center[**Model Results on Test Dataset (S = 10)**]
  
| `\(y_{actual}\)` | `\(y_{pred}\)` | *Type*                                       |
| ------------ | ---------- | -------------------------------------------- |
|            1 |          1 | `\(\color{green}{\text{True Positive (TP)}}\)`   |
|            1 |          0 | `\(\color{red}{\text{False Negative (FN)}}\)`    |
|            0 |          0 | `\(\color{blue}{\text{True Negative (TN)}}\)`    |
|            0 |          1 | `\(\color{orange}{\text{False Positive (FP)}}\)` |
|            0 |          0 | |
|            1 |          1 | |
|            0 |          0 | |
|            0 |          1 | |
|            0 |          0 | |
|            1 |          1 | &lt;p&gt;&lt;/p&gt; |

]

.right-60[

.center[**Confusion Matrix**]
  
.pull-left[

| *Counts*         | `\(y_{pred} = 1\)`               | `\(y_{pred} = 0\)`               |
| ---------------- | :--------------------------: | :--------------------------: |
| `\(y_{actual} = 1\)` | `\(\color{green}{\text{#TP}}\)`  | `\(\color{red}{\text{#FN}}\)`    |
| `\(y_{actual} = 0\)` | `\(\color{orange}{\text{#FP}}\)` | `\(\color{blue}{\text{#TN}}\)`   |

]
  
.pull-right[

| *Ratios*         | `\(y_{pred} = 1\)`                 | `\(y_{pred} = 0\)`                 |
| ---------------- | :----------------------------: | :----------------------------: |
| `\(y_{actual} = 1\)` | `\(\color{green}{\text{#TP/S}}\)`  | `\(\color{red}{\text{#FN/S}}\)`    |
| `\(y_{actual} = 0\)` | `\(\color{orange}{\text{#FP/S}}\)` | `\(\color{blue}{\text{#TN/S}}\)`   |

]
  
&lt;br&gt;
  
**Q1:** Which dataset(s) should we be looking at here (train, test, both)?
  
**Q2:** Of the 4 types (TP, FN, TN, FP), how many and which ones are errors? Are the different types of errors equally bad? Or can some be worse?
  
**Q3:** Why/When use these two Confusion Matrix versions (Counts, Ratio)? 

]

---

# Confusion Matrix - Activity - Solutions

.left-40[

.center[**Model Results on Test Dataset (S = 10)**]
  
| `\(y_{actual}\)` | `\(y_{pred}\)` | *Type*                                       |
| ------------ | ---------- | -------------------------------------------- |
|            1 |          1 | `\(\color{green}{\text{True Positive (TP)}}\)`   |
|            1 |          0 | `\(\color{red}{\text{False Negative (FN)}}\)`    |
|            0 |          0 | `\(\color{blue}{\text{True Negative (TN)}}\)`    |
|            0 |          1 | `\(\color{orange}{\text{False Positive (FP)}}\)` |
|            0 |          0 | `\(\color{blue}{\text{TN}}\)`   |
|            1 |          1 | `\(\color{green}{\text{TP}}\)`  |
|            0 |          0 | `\(\color{blue}{\text{TN}}\)`   |
|            0 |          1 | `\(\color{orange}{\text{FP}}\)` |
|            0 |          0 | `\(\color{blue}{\text{TN}}\)`   |
|            1 |          1 | `\(\color{green}{\text{TP}}\)`  |

]

.right-60[

.center[**Confusion Matrix**]
  
.pull-left[

| *Counts*         | `\(y_{pred} = 1\)`      | `\(y_{pred} = 0\)`      |
| ---------------- | :-----------------: | :-----------------: |
| `\(y_{actual} = 1\)` | `\(\color{green}{3}\)`  | `\(\color{red}{1}\)`    |
| `\(y_{actual} = 0\)` | `\(\color{orange}{2}\)` | `\(\color{blue}{4}\)`   |

]
  
.pull-right[

| *Ratios*         | `\(y_{pred} = 1\)`        | `\(y_{pred} = 0\)`        |
| ---------------- | :-------------------: | :-------------------: |
| `\(y_{actual} = 1\)` | `\(\color{green}{0.3}\)`  | `\(\color{red}{0.1}\)`    |
| `\(y_{actual} = 0\)` | `\(\color{orange}{0.2}\)` | `\(\color{blue}{0.4}\)`   |

]
  
&lt;br&gt;
  
**Q1:** Which dataset(s) should we be looking at here (train, test, both)?
  
**Q2:** Of the 4 types (TP, FN, TN, FP), how many and which ones are errors? Are the different types of errors equally bad? Or can some be worse?
  
**Q3:** Why/When use these two Confusion Matrix versions (Counts, Ratio)? 

]


---

# Confusion Matrix - Code - R

.small[

.pull-left[


```r
y_actual = factor(c(1, 0, 0, 1, 0, 1, 0, 0, 0, 1))
y_pred   = factor(c(1, 0, 1, 0, 0, 1, 0, 1, 0, 1))
table(y_actual, y_pred)
```

```
##         y_pred
## y_actual 0 1
##        0 4 2
##        1 1 3
```


```r
table(y_pred, y_actual)
```

```
##       y_actual
## y_pred 0 1
##      0 4 1
##      1 2 3
```


```r
library(caret)
cm = confusionMatrix(data=y_pred, reference=y_actual)
cm$table
```

```
##           Reference
## Prediction 0 1
##          0 4 1
##          1 2 3
```



]

.pull-right[


```r
library(cvms)
cm_df = data.frame(cm$table)
names(cm_df) = c('Prediction', 'Target', 'N')
plot_confusion_matrix(cm_df)
```

&lt;img src="presentation_files/figure-html/unnamed-chunk-5-1.png" width="80%" /&gt;

]

]

---

# Confusion Matrix - Code - Python

&lt;iframe src="python-notebooks/confusion-matrix.html" height="530" width=700"&gt;&lt;/iframe&gt;

---

# Metrics - Confusion Matrix

| Total = P + N                             | Predicted Positive &lt;br&gt; `\(y_{pred} = 1\)`       | Predicted Negative &lt;br&gt; `\(y_{pred} = 0\)`    |          |
| :---------------------------------------: | :------------------------------------------: | :---------------------------------------: | -------- |
| **Actual Positive** &lt;br&gt; `\(y_{actual} = 1\)` | `\(\color{green}{\text{True Positive (TP)}}\)`   | `\(\color{red}{\text{False Negative (FN)}}\)` | Recall = TPR = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{red}{\text{FN}} }\)` |
| **Actual Negative** &lt;br&gt; `\(y_{actual} = 0\)` | `\(\color{orange}{\text{False Positive (FP)}}\)` | `\(\color{blue}{\text{True Negative (TN)}}\)` | Specificity = TNR = `\(\frac{ \color{blue}{\text{TN}} }{ \color{orange}{\text{FP}} + \color{blue}{\text{TN}} }\)` |
|                                           | Precision = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{orange}{\text{FP}} }\)` | | Accuracy = `\(\frac{ \color{green}{\text{TP}} + \color{blue}{\text{TN}} }{ \color{green}{\text{TP}} + \color{blue}{\text{TN}} + \color{orange}{\text{FP}} + \color{red}{\text{FN}} }\)` |

&lt;br&gt;

| Metric        | Definition / Meaning    |
| ------------- | ----------------------- |
| **Accuracy**  | proportion of all cases classified correctly |
| **Precision** | proportion of predicted positives that are actual positives |
| **Recall** (*Sensitivity*, *TPR*) | proportion of actual positives classified correctly |
| **Specificity** (*Selectivity*, *TNR*) | proportion of actual negatives classified correctly |

Others: ROC AUC, Precision-Recall AUC, F1 score

---

# Metrics - Confusion Matrix - Activity (5 min)

| Total = P + N                             | Predicted Positive &lt;br&gt; `\(y_{pred} = 1\)`       | Predicted Negative &lt;br&gt; `\(y_{pred} = 0\)`    |          |
| :---------------------------------------: | :------------------------------------------: | :---------------------------------------: | -------- |
| **Actual Positive** &lt;br&gt; `\(y_{actual} = 1\)` | `\(\color{green}{\text{TP} = 3}\)`   | `\(\color{red}{\text{FN} = 1}\)` | Recall = TPR = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{red}{\text{FN}} }\)` = **???** |
| **Actual Negative** &lt;br&gt; `\(y_{actual} = 0\)` | `\(\color{orange}{\text{FP} = 2}\)` | `\(\color{blue}{\text{TN} = 4}\)` | Specificity = TNR = `\(\frac{ \color{blue}{\text{TN}} }{ \color{orange}{\text{FP}} + \color{blue}{\text{TN}} }\)` = **???** |
|                                           | Precision = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{orange}{\text{FP}} }\)` = **???** | | Accuracy = `\(\frac{ \color{green}{\text{TP}} + \color{blue}{\text{TN}} }{ \color{green}{\text{TP}} + \color{blue}{\text{TN}} + \color{orange}{\text{FP}} + \color{red}{\text{FN}} }\)` = **???** |

---

# Metrics - Confusion Matrix - Activity - Solution

| Total = P + N                             | Predicted Positive &lt;br&gt; `\(y_{pred} = 1\)`       | Predicted Negative &lt;br&gt; `\(y_{pred} = 0\)`    |          |
| :---------------------------------------: | :------------------------------------------: | :---------------------------------------: | -------- |
| **Actual Positive** &lt;br&gt; `\(y_{actual} = 1\)` | `\(\color{green}{\text{TP} = 3}\)`   | `\(\color{red}{\text{FN} = 1}\)` | Recall = TPR = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{red}{\text{FN}} }\)` = **0.75** |
| **Actual Negative** &lt;br&gt; `\(y_{actual} = 0\)` | `\(\color{orange}{\text{FP} = 2}\)` | `\(\color{blue}{\text{TN} = 4}\)` | Specificity = TNR = `\(\frac{ \color{blue}{\text{TN}} }{ \color{orange}{\text{FP}} + \color{blue}{\text{TN}} }\)` = **0.67** |
|                                           | Precision = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{orange}{\text{FP}} }\)` = **0.60** | | Accuracy = `\(\frac{ \color{green}{\text{TP}} + \color{blue}{\text{TN}} }{ \color{green}{\text{TP}} + \color{blue}{\text{TN}} + \color{orange}{\text{FP}} + \color{red}{\text{FN}} }\)` = **0.70** |

---

# Metrics - Confusion Matrix - Code - R

.pull-left[


```r
confusionMatrix(data=y_pred, reference=y_actual, 
                positive='1')
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction 0 1
##          0 4 1
##          1 2 3
##                                           
##                Accuracy : 0.7             
##                  95% CI : (0.3475, 0.9333)
##     No Information Rate : 0.6             
##     P-Value [Acc &gt; NIR] : 0.3823          
##                                           
##                   Kappa : 0.4             
##                                           
##  Mcnemar's Test P-Value : 1.0000          
##                                           
##             Sensitivity : 0.7500          
##             Specificity : 0.6667          
##          Pos Pred Value : 0.6000          
##          Neg Pred Value : 0.8000          
##              Prevalence : 0.4000          
##          Detection Rate : 0.3000          
##    Detection Prevalence : 0.5000          
##       Balanced Accuracy : 0.7083          
##                                           
##        'Positive' Class : 1               
## 
```

]

.pull-right[


```r
confusionMatrix(data=y_pred, reference=y_actual, 
                positive='1', mode="prec_recall")
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction 0 1
##          0 4 1
##          1 2 3
##                                           
##                Accuracy : 0.7             
##                  95% CI : (0.3475, 0.9333)
##     No Information Rate : 0.6             
##     P-Value [Acc &gt; NIR] : 0.3823          
##                                           
##                   Kappa : 0.4             
##                                           
##  Mcnemar's Test P-Value : 1.0000          
##                                           
##               Precision : 0.6000          
##                  Recall : 0.7500          
##                      F1 : 0.6667          
##              Prevalence : 0.4000          
##          Detection Rate : 0.3000          
##    Detection Prevalence : 0.5000          
##       Balanced Accuracy : 0.7083          
##                                           
##        'Positive' Class : 1               
## 
```

]

---

# Metrics - Confusion Matrix - Code - Python

&lt;iframe src="python-notebooks/metrics-confusion-matrix.html" height="530" width=700"&gt;&lt;/iframe&gt;

---

# Metrics - ROC AUC

.pull-left[

&lt;img src="figures/Roc-draft-xkcd-style.svg" width="75%" height="75%"&gt;

**Receiver Operating Characteristic (ROC) curve**

- Sensitivity vs Specificity, by discrimination threshold
- Y axis: **TPR** (*Recall*, *Sensitivity*) = `\(\frac{ \color{green}{\text{TP}} }{ \text{P} }\)` = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{red}{\text{FN}} }\)`
- X axis (*2 versions*):
  - **FPR** = 1 - Specificity = `\(\frac{ \color{orange}{\text{FP}} }{ \text{N} }\)` = `\(\frac{ \color{orange}{\text{FP}} }{ \color{orange}{\text{FP}} + \color{blue}{\text{TN}} }\)`
  - TNR = Specificity = `\(\frac{ \color{blue}{\text{TN}} }{ \text{N} }\)` = `\(\frac{ \color{blue}{\text{TN}} }{ \color{orange}{\text{FP}} + \color{blue}{\text{TN}} }\)`

]

.pull-right[

&lt;img src="figures/sphx_glr_plot_roc_0011.png" width="75%" height="75%"&gt;

**Area under the ROC curve (ROC AUC)**

- Higher ROC AUC =&gt; Better (more effective) classifier
- ROC AUC = 1 for perfect classifier
- ROC AUC = 0.5 for random (ineffective) classifier
- Robust metric for evaluating/comparing classifier(s)

&lt;span style="font-size:70%; font-style: italic;"&gt;*Image sources: [Wikipedia / xkcd](https://en.wikipedia.org/wiki/File:Roc-draft-xkcd-style.svg) (left) [sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) (right)*&lt;/span&gt;

]

---

# Metrics - Precision-Recall

.pull-left[

**Precision** = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{orange}{\text{FP}} }\)`

- ratio of predicted positives that are actual positives
- ability of not classifying negative samples as positive

**Recall** (*Sensitivity*, *TPR*) = `\(\frac{ \color{green}{\text{TP}} }{ \color{green}{\text{TP}} + \color{red}{\text{FN}} }\)`

- ratio of actual positives classified correctly
- ability of the classifier to find all the positive samples

**F1 score**: Harmonic mean of Precision and Recall:

- `\(\text{F1 score} = 2 \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\)`

**Trade-off** between the two error types:

* *Type I error:* `\(\color{orange}{\text{False Positive (FP)}}\)`
* *Type II error:* `\(\color{red}{\text{False Negative (FN)}}\)`


Image source: [sklearn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#precision-recall)

]

.pull-right[

&lt;img src="figures/sphx_glr_plot_precision_recall_001.png" width="80%" height="80%"&gt;

**Precision-Recall curve**

- Precision (Y) vs Recall (X) by discriminant threshold
- **Average Precision (AP):** weighted mean of precisions
- **Precision-Recall AUC:** area under the curve
- Robust metric, useful for imbalanced classes

]

---

# Summary &amp; Extra Notes

.pull-left[

.center[**Summary**]

Introduction

- Supervised Machine Learning, Classification
- Evaluating Classification Models

4 Outcome &amp; 2 Error Types

Confusion Matrix

Simpler Metrics: Accuracy, Precision, Recall, Specificity

Robust/Trade-off Metrics:

- ROC curve &amp; ROC AUC
- Precision-Recall:
  - F1 score
  - Precision-Recall curve: AP &amp; AUC

]

--

.pull-right[

.center[**Extra Notes**]

Imbalanced classification:

* Different frequencies for different classes
* Use Robust/Trade-off metrics in model evaluation
* Python:
  * `sklearn`: use `class_weight = 'balanced'` in some algorithms (e.g., Logistic Regression)
  * `imbalanced-learn`: more advanced (over/under) sampling algorithms

Multiple classes:

* We looked at Binary Classification (2 classes)
* The concepts generalize for multiple classes

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>
<style>
.logo {
  background-image: url("logo-white.png");
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 15%;
  height: 15%;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
